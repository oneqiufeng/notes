- [ ] [gh物体垂直相机视角](https://www.xiaohongshu.com/explore/6575cf59000000003403c003)
- [ ] [[供需交互视角下城市公园分布空间公平性评估]]
- [ ] 【NLP系列】Transformer复现(10分钟版本)
- [ ] 《[[Attention Is All You Need]]》
- [ ] Transformer创新点：简化版Transformer

论文的评价很高，代码链接已公开  
来自 ETH Zurich 的研究者讨论了如何在不影响收敛特性和下游任务性能的情况下简化 LLM 所必需的标准 Transformer 块。基于信号传播理论和经验证据，他们发现可以移除一些部分，比如残差连接、归一化层（LayerNorm）、投影和值参数以及 MLP 序列化子块（有利于并行布局），以简化类似 GPT 的解码器架构以及编码器式 BERT 模型。  
对于每个涉及的组件，研究者都探讨了是否可以在不降低训练速度的情况下将其移除（包括每次更新步骤和运行时间），以及为此需要 Transformer 块进行哪些架构修改。

论文链接: https://arxiv.org/pdf/2311.01906.pdf
代码链接: https://github.com/bobby-he/simplified_transformers

![[../../归档/picture/Transformer创新点：简化版Transformer_1_文BamBoo_来自小红书网页版.jpg]]

![[../../归档/picture/Transformer创新点：简化版Transformer_2_文BamBoo_来自小红书网页版.jpg]]